{"cells":[{"cell_type":"markdown","id":"b74ecd07","metadata":{"id":"b74ecd07"},"source":["In this project, we opted to assess the reliability of the ```WEAT``` by employing ```word2vec``` embeddings. To facilitate the process, we specifically selected the embeddings that were pretrained on Google News, known as ```word2vec-google-news-300```."]},{"cell_type":"markdown","id":"7bd34266","metadata":{"id":"7bd34266"},"source":["Frist, we run WEAT on the pretrained word2vec embeddings for gender bias:"]},{"cell_type":"code","source":["!pip install wefe"],"metadata":{"id":"O6rQADi-IiiZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685442639586,"user_tz":-120,"elapsed":12675,"user":{"displayName":"junoprent@live.nl","userId":"17636247835891681767"}},"outputId":"b41e21f1-b3a7-40ca-e202-1e1ecd8bed48"},"id":"O6rQADi-IiiZ","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wefe\n","  Downloading wefe-0.4.1-py3-none-any.whl (7.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from wefe) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from wefe) (1.10.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from wefe) (1.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from wefe) (1.5.3)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from wefe) (4.3.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from wefe) (5.13.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from wefe) (1.16.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wefe) (2.27.1)\n","Collecting semantic-version (from wefe)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from wefe) (4.65.0)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->wefe) (6.3.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->wefe) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->wefe) (2022.7.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->wefe) (8.2.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wefe) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wefe) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->wefe) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wefe) (3.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->wefe) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->wefe) (3.1.0)\n","Installing collected packages: semantic-version, wefe\n","Successfully installed semantic-version-2.10.0 wefe-0.4.1\n"]}]},{"cell_type":"code","execution_count":2,"id":"09258618","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09258618","executionInfo":{"status":"ok","timestamp":1685443085488,"user_tz":-120,"elapsed":445906,"user":{"displayName":"junoprent@live.nl","userId":"17636247835891681767"}},"outputId":"199e1eb6-81df-4672-b890-04c837f24aa8"},"outputs":[{"output_type":"stream","name":"stdout","text":["[=================================================-] 99.9% 1661.9/1662.8MB downloaded\n"]}],"source":["from wefe.word_embedding_model import WordEmbeddingModel\n","from wefe.query import Query\n","from wefe.metrics import WEAT\n","import gensim.downloader as api\n","\n","import pandas as pd\n","import numpy as np\n","\n","word2vec_model = WordEmbeddingModel(api.load('word2vec-google-news-300'),\n","                                    'word2vec-google-news-300')"]},{"cell_type":"code","execution_count":3,"id":"5d271f03","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5d271f03","executionInfo":{"status":"ok","timestamp":1685443085489,"user_tz":-120,"elapsed":11,"user":{"displayName":"junoprent@live.nl","userId":"17636247835891681767"}},"outputId":"0517afca-4156-46bf-ea08-e8e821f3e673"},"outputs":[{"output_type":"stream","name":"stdout","text":["32 32 32 32\n"]}],"source":["# target lists: lists of 32 most common names in the US, as per \n","# https://www.ssa.gov/oact/babynames/decades/century.html\n","male_names = [\n","    \"James\", \"Robert\", \"John\", \"Michael\", \"David\", \"William\", \"Richard\", \n","    \"Joseph\", \"Thomas\", \"Christopher\", \"Charles\", \"Daniel\", \"Matthew\",\n","    \"Anthony\", \"Mark\", \"Donald\", \"Steven\", \"Andrew\", \"Paul\", \"Joshua\",\n","    \"Kenneth\", \"Kevin\", \"Brian\", \"George\", \"Timothy\", \"Ronald\", \"Jason\",\n","    \"Edward\", \"Jeffrey\", \"Ryan\", \"Jacob\", \"Gary\"\n","]\n","\n","female_names = [\n","    \"Mary\", \"Patricia\", \"Jennifer\", \"Linda\", \"Elizabeth\", \"Barbara\", \"Susan\",\n","    \"Jessica\", \"Sarah\", \"Karen\", \"Lisa\", \"Nancy\", \"Betty\", \"Sandra\", \"Margaret\",\n","    \"Ashley\", \"Kimberly\", \"Emily\", \"Donna\", \"Michelle\", \"Carol\", \"Amanda\",\n","    \"Melissa\", \"Deborah\", \"Stephanie\", \"Dorothy\", \"Rebecca\", \"Sharon\", \"Laura\",\n","    \"Cynthia\", \"Amy\", \"Kathleen\"\n","]\n","\n","#attribute sets\n","career_names = [\n","    \"Engineer\", \"Doctor\", \"Teacher\", \"Lawyer\", \"Nurse\", \"Programmer\", \"Artist\",\n","    \"Scientist\", \"Writer\", \"Chef\", \"Athlete\", \"Architect\", \"Musician\",\n","    \"Officer\", \"Firefighter\", \"Pilot\", \"Psychologist\", \"Entrepreneur\",\n","    \"Veterinarian\", \"Dentist\", \"Actor\", \"Designer\", \"Photographer\",\n","    \"Journalist\", \"Engineer\", \"Professor\", \"Economist\", \"Researcher\",\n","    \"Accountant\", \"Electrician\", \"Mechanic\", \"Secretary\"\n","]\n","\n","family_names = [\n","    \"Family\", \"Parents\", \"Children\", \"Siblings\", \"Mother\", \"Father\", \"Sister\",\n","    \"Brother\", \"Daughter\", \"Son\", \"Grandparents\", \"Grandmother\", \"Grandfather\",\n","    \"Granddaughter\", \"Grandson\", \"Aunt\", \"Uncle\", \"Cousin\", \"Niece\", \"Fiance\",\n","    \"Fiancee\", \"Spouse\", \"Husband\", \"Wife\", \"Stepmother\", \"Stepfather\",\n","    \"Stepsister\", \"Stepbrother\", \"Stepdaughter\", \"Stepson\", \"Godmother\", \n","    \"Godfather\"\n","]\n","\n","print(len(male_names), len(female_names), len(career_names), len(family_names))"]},{"cell_type":"markdown","id":"b9197e32","metadata":{"id":"b9197e32"},"source":["The ```effect_size``` quantifies the magnitude or strength of the association between the target and attribute concepts in the WEAT. A larger effect size suggests a stronger association. In this case, the effect size is 1.9518473221010546, indicating a relatively large effect. However, ```p_value``` is nan indicating p-value could not be calculated or is undefined"]},{"cell_type":"markdown","id":"829882fc","metadata":{"id":"829882fc"},"source":["In the subsequent steps, we examine both the reliability and validity of the WEAT. To assess reliability, we divide the aforementioned four lists ```male_names```, ```female_names```, ```career```,```family``` into four sublists and perform the WEAT on each of them to determine if the results exhibit consistency across all sublists. Regarding validity, we compare the WEAT outcomes with 2 alternative bias measurements ```Word Analogy Testing``` and ```Word Similarity Comparison```, . This comparative analysis allows us to evaluate the validity of the WEAT by examining the consistency of results across these diverse measurements. Furthermore, we incorporated a downstream task called the ```semantic textual similarity task``` as part of the validity assessment. This additional task serves as further evidence to support the evaluation of the WEAT's validity."]},{"cell_type":"markdown","id":"bad3d17d","metadata":{"id":"bad3d17d"},"source":["## RELIABILITY"]},{"cell_type":"code","execution_count":12,"id":"1dc8fbe6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1dc8fbe6","executionInfo":{"status":"ok","timestamp":1685457912736,"user_tz":-120,"elapsed":10928220,"user":{"displayName":"junoprent@live.nl","userId":"17636247835891681767"}},"outputId":"55fa22fa-20c7-444b-d20e-18e49aeabb88"},"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline: \n"," effect_size: 1.2198651231838853, WEAT: 1.2642560498432196\n","Results for random sample of size 4\n"," average effect_size: 0.9269837420264228, average WEAT: 0.1678243221731313\n","Results for random sample of size 5\n"," average effect_size: 0.9802894599706243, average WEAT: 0.20864114190928176\n","Results for random sample of size 6\n"," average effect_size: 1.012864899947951, average WEAT: 0.24928547117730135\n","Results for random sample of size 7\n"," average effect_size: 1.0441179468801016, average WEAT: 0.2895405046687927\n","Results for random sample of size 8\n"," average effect_size: 1.0790689899797556, average WEAT: 0.331844707325648\n","Results for random sample of size 9\n"," average effect_size: 1.0877671865686183, average WEAT: 0.3690776558700199\n","Results for random sample of size 10\n"," average effect_size: 1.1125678883620385, average WEAT: 0.4120753589198836\n","Results for random sample of size 11\n"," average effect_size: 1.1183082669229727, average WEAT: 0.4474134034733047\n","Results for random sample of size 12\n"," average effect_size: 1.138668564345386, average WEAT: 0.49235311931476256\n","Results for random sample of size 13\n"," average effect_size: 1.1511314945470095, average WEAT: 0.5344684145938187\n","Results for random sample of size 14\n"," average effect_size: 1.1635751865844897, average WEAT: 0.5763966914115354\n","Results for random sample of size 15\n"," average effect_size: 1.1687020413579567, average WEAT: 0.6128721763993632\n","Results for random sample of size 16\n"," average effect_size: 1.1708051322386226, average WEAT: 0.6498914479950715\n","Results for random sample of size 17\n"," average effect_size: 1.1809810632610824, average WEAT: 0.6908888765582195\n","Results for random sample of size 18\n"," average effect_size: 1.1874962702094427, average WEAT: 0.7316931172688068\n","Results for random sample of size 19\n"," average effect_size: 1.1938652677601906, average WEAT: 0.7714634416079205\n","Results for random sample of size 20\n"," average effect_size: 1.1943725481187095, average WEAT: 0.8077329120791349\n","Results for random sample of size 21\n"," average effect_size: 1.200406608293266, average WEAT: 0.8482267557211857\n","Results for random sample of size 22\n"," average effect_size: 1.2022399683570304, average WEAT: 0.8866825315926293\n","Results for random sample of size 23\n"," average effect_size: 1.2041968974248292, average WEAT: 0.9239294317948737\n","Results for random sample of size 24\n"," average effect_size: 1.206033676629976, average WEAT: 0.9607608408351279\n"]}],"source":["weat = WEAT()\n","\n","gender_occupation_query = Query([male_names, female_names],\n","                                [career_names, family_names],\n","                                ['Male names', 'Female names'],\n","                                ['Career', 'Family'])\n","\n","baseline = {'effect_size': weat.run_query(gender_occupation_query, word2vec_model)['effect_size'],\n","            'weat': weat.run_query(gender_occupation_query, word2vec_model)['weat']}\n","print(f\"Baseline: \\n effect_size: {baseline['effect_size']}, WEAT: {baseline['weat']}\")\n","\n","scores = {}\n","\n","for i in range(4, 25):\n","    scores[i] = {'effect_size':[], 'weat': []}\n","    for _ in range(10000):\n","        male_name_sublist = np.random.choice(male_names, size=i, replace=False)\n","        female_name_sublist = np.random.choice(female_names, size=i, replace=False)\n","        career_sublist = np.random.choice(career_names, size=i, replace=False)\n","        family_sublist = np.random.choice(family_names, size=i, replace=False)\n","\n","        gender_occupation_query = Query([male_name_sublist, female_name_sublist],\n","                                    [career_sublist, family_sublist],\n","                                    ['Male names', 'Female names'],\n","                                    ['Career', 'Family'])\n","        \n","        result = weat.run_query(gender_occupation_query, word2vec_model, lost_vocabulary_threshold=0.25)\n","        scores[i]['effect_size'].append(result['effect_size'])\n","        scores[i]['weat'].append(result['weat'])\n","\n","    print(f\"Results for random sample of size {i}\\n average effect_size: {np.mean(scores[i]['effect_size'])}, average WEAT: {np.mean(scores[i]['weat'])}\")"]},{"cell_type":"code","source":["avg_effect_sizes = [np.mean(scores[i]['effect_size']) for i in range(4, 25)]\n","avg_weats = [np.mean(scores[i]['weat']) for i in range(4, 25)]\n","\n","df = pd.DataFrame({'sample size': ['baseline'] + list(range(4, 25)),\n","                   'average effect size': [baseline['effect_size']] + avg_effect_sizes,\n","                   'average WEAT': [baseline['weat']] + avg_weats})\n","\n","df.style.set_caption('Averages after 10000 iterations').hide(axis='index')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":756},"id":"rPl0OpTIEvuQ","executionInfo":{"status":"ok","timestamp":1685459565420,"user_tz":-120,"elapsed":436,"user":{"displayName":"junoprent@live.nl","userId":"17636247835891681767"}},"outputId":"928c23a0-7313-44cb-e745-47d87bcb5fa4"},"id":"rPl0OpTIEvuQ","execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7f50c2f74d90>"],"text/html":["<style type=\"text/css\">\n","</style>\n","<table id=\"T_c9cd9\" class=\"dataframe\">\n","  <caption>Averages after 10000 iterations</caption>\n","  <thead>\n","    <tr>\n","      <th id=\"T_c9cd9_level0_col0\" class=\"col_heading level0 col0\" >sample size</th>\n","      <th id=\"T_c9cd9_level0_col1\" class=\"col_heading level0 col1\" >average effect size</th>\n","      <th id=\"T_c9cd9_level0_col2\" class=\"col_heading level0 col2\" >average WEAT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_c9cd9_row0_col0\" class=\"data row0 col0\" >baseline</td>\n","      <td id=\"T_c9cd9_row0_col1\" class=\"data row0 col1\" >1.219865</td>\n","      <td id=\"T_c9cd9_row0_col2\" class=\"data row0 col2\" >1.264256</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row1_col0\" class=\"data row1 col0\" >4</td>\n","      <td id=\"T_c9cd9_row1_col1\" class=\"data row1 col1\" >0.926984</td>\n","      <td id=\"T_c9cd9_row1_col2\" class=\"data row1 col2\" >0.167824</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row2_col0\" class=\"data row2 col0\" >5</td>\n","      <td id=\"T_c9cd9_row2_col1\" class=\"data row2 col1\" >0.980289</td>\n","      <td id=\"T_c9cd9_row2_col2\" class=\"data row2 col2\" >0.208641</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row3_col0\" class=\"data row3 col0\" >6</td>\n","      <td id=\"T_c9cd9_row3_col1\" class=\"data row3 col1\" >1.012865</td>\n","      <td id=\"T_c9cd9_row3_col2\" class=\"data row3 col2\" >0.249285</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row4_col0\" class=\"data row4 col0\" >7</td>\n","      <td id=\"T_c9cd9_row4_col1\" class=\"data row4 col1\" >1.044118</td>\n","      <td id=\"T_c9cd9_row4_col2\" class=\"data row4 col2\" >0.289541</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row5_col0\" class=\"data row5 col0\" >8</td>\n","      <td id=\"T_c9cd9_row5_col1\" class=\"data row5 col1\" >1.079069</td>\n","      <td id=\"T_c9cd9_row5_col2\" class=\"data row5 col2\" >0.331845</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row6_col0\" class=\"data row6 col0\" >9</td>\n","      <td id=\"T_c9cd9_row6_col1\" class=\"data row6 col1\" >1.087767</td>\n","      <td id=\"T_c9cd9_row6_col2\" class=\"data row6 col2\" >0.369078</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row7_col0\" class=\"data row7 col0\" >10</td>\n","      <td id=\"T_c9cd9_row7_col1\" class=\"data row7 col1\" >1.112568</td>\n","      <td id=\"T_c9cd9_row7_col2\" class=\"data row7 col2\" >0.412075</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row8_col0\" class=\"data row8 col0\" >11</td>\n","      <td id=\"T_c9cd9_row8_col1\" class=\"data row8 col1\" >1.118308</td>\n","      <td id=\"T_c9cd9_row8_col2\" class=\"data row8 col2\" >0.447413</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row9_col0\" class=\"data row9 col0\" >12</td>\n","      <td id=\"T_c9cd9_row9_col1\" class=\"data row9 col1\" >1.138669</td>\n","      <td id=\"T_c9cd9_row9_col2\" class=\"data row9 col2\" >0.492353</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row10_col0\" class=\"data row10 col0\" >13</td>\n","      <td id=\"T_c9cd9_row10_col1\" class=\"data row10 col1\" >1.151131</td>\n","      <td id=\"T_c9cd9_row10_col2\" class=\"data row10 col2\" >0.534468</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row11_col0\" class=\"data row11 col0\" >14</td>\n","      <td id=\"T_c9cd9_row11_col1\" class=\"data row11 col1\" >1.163575</td>\n","      <td id=\"T_c9cd9_row11_col2\" class=\"data row11 col2\" >0.576397</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row12_col0\" class=\"data row12 col0\" >15</td>\n","      <td id=\"T_c9cd9_row12_col1\" class=\"data row12 col1\" >1.168702</td>\n","      <td id=\"T_c9cd9_row12_col2\" class=\"data row12 col2\" >0.612872</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row13_col0\" class=\"data row13 col0\" >16</td>\n","      <td id=\"T_c9cd9_row13_col1\" class=\"data row13 col1\" >1.170805</td>\n","      <td id=\"T_c9cd9_row13_col2\" class=\"data row13 col2\" >0.649891</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row14_col0\" class=\"data row14 col0\" >17</td>\n","      <td id=\"T_c9cd9_row14_col1\" class=\"data row14 col1\" >1.180981</td>\n","      <td id=\"T_c9cd9_row14_col2\" class=\"data row14 col2\" >0.690889</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row15_col0\" class=\"data row15 col0\" >18</td>\n","      <td id=\"T_c9cd9_row15_col1\" class=\"data row15 col1\" >1.187496</td>\n","      <td id=\"T_c9cd9_row15_col2\" class=\"data row15 col2\" >0.731693</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row16_col0\" class=\"data row16 col0\" >19</td>\n","      <td id=\"T_c9cd9_row16_col1\" class=\"data row16 col1\" >1.193865</td>\n","      <td id=\"T_c9cd9_row16_col2\" class=\"data row16 col2\" >0.771463</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row17_col0\" class=\"data row17 col0\" >20</td>\n","      <td id=\"T_c9cd9_row17_col1\" class=\"data row17 col1\" >1.194373</td>\n","      <td id=\"T_c9cd9_row17_col2\" class=\"data row17 col2\" >0.807733</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row18_col0\" class=\"data row18 col0\" >21</td>\n","      <td id=\"T_c9cd9_row18_col1\" class=\"data row18 col1\" >1.200407</td>\n","      <td id=\"T_c9cd9_row18_col2\" class=\"data row18 col2\" >0.848227</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row19_col0\" class=\"data row19 col0\" >22</td>\n","      <td id=\"T_c9cd9_row19_col1\" class=\"data row19 col1\" >1.202240</td>\n","      <td id=\"T_c9cd9_row19_col2\" class=\"data row19 col2\" >0.886683</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row20_col0\" class=\"data row20 col0\" >23</td>\n","      <td id=\"T_c9cd9_row20_col1\" class=\"data row20 col1\" >1.204197</td>\n","      <td id=\"T_c9cd9_row20_col2\" class=\"data row20 col2\" >0.923929</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c9cd9_row21_col0\" class=\"data row21 col0\" >24</td>\n","      <td id=\"T_c9cd9_row21_col1\" class=\"data row21 col1\" >1.206034</td>\n","      <td id=\"T_c9cd9_row21_col2\" class=\"data row21 col2\" >0.960761</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["# df.to_csv('weat_reliability.csv', index=False)"],"metadata":{"id":"pdEEdDpWOBQL"},"id":"pdEEdDpWOBQL","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"477fb50f","metadata":{"id":"477fb50f"},"source":["### VALIDITY"]},{"cell_type":"markdown","id":"c4fc6b32","metadata":{"id":"c4fc6b32"},"source":["#### 1. Word Analogy Testing "]},{"cell_type":"code","execution_count":null,"id":"5c1fcca0","metadata":{"id":"5c1fcca0"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","from gensim.models.word2vec import Word2Vec\n","import gensim.downloader as api\n","\n","## load pretrained word2vec model\n","model_glove_twitter = api.load('word2vec-google-news-300')"]},{"cell_type":"code","execution_count":null,"id":"efddc17d","metadata":{"id":"efddc17d","outputId":"0311ecd6-7aa7-4f1c-c250-1c84f347f7ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["man->woman  || king->queen\n","father->mother  || son->daughter\n","brother->sister  || uncle->aunt\n","boy->girl  || prince->princess\n","he->she  || king->queen\n","he->she  || pilot->flight_attendant\n","man->woman  || director->chairwoman\n","father->mother  || leader->Leader\n","husband->wife  || king->kings\n"]}],"source":["# Define word analogies\n","analogies = [\n","    ('man', 'woman', 'king'),\n","    ('father', 'mother', 'son'),\n","    ('brother', 'sister', 'uncle'),\n","    ('boy', 'girl', 'prince'),\n","    ('he', 'she', 'king'),\n","    ('he', 'she', 'pilot'),\n","    ('man', 'woman', 'director'),\n","    ('father', 'mother', 'leader'),\n","    ('husband', 'wife', 'king')\n","]\n","# Perform word analogy testing\n","for analogy in analogies:\n","    a, b, c = analogy\n","    predicted_word = model_glove_twitter.most_similar(positive=[b, c], negative=[a])[0][0]\n","\n","    print(f\"{a}->{b}  || {c}->{predicted_word}\")"]},{"cell_type":"code","execution_count":null,"id":"a36d8587","metadata":{"id":"a36d8587"},"outputs":[],"source":["rater1 = [1,2,3,4,5,6,7,0,0]"]},{"cell_type":"markdown","id":"cd412e13","metadata":{"id":"cd412e13"},"source":["#### 2. Word Similarity Comparison"]},{"cell_type":"code","execution_count":null,"id":"ef3d793d","metadata":{"id":"ef3d793d"},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","from scipy.stats import ttest_ind"]},{"cell_type":"code","execution_count":null,"id":"12cd68a2","metadata":{"id":"12cd68a2"},"outputs":[],"source":["# Define gendered word pairs\n","gender_pairs = [\n","    ('man', 'woman'),\n","    ('father', 'mother'),\n","    ('son', 'daughter'),\n","    ('brother', 'sister'),\n","    ('uncle', 'aunt'),\n","    ('nephew', 'niece'),\n","    ('king', 'queen'),\n","    ('prince', 'princess'),\n","    ('emperor', 'empress'),\n","    ('god', 'goddess'),\n","    ('male', 'female'),\n","    ('boy', 'girl'),\n","    ('groom', 'bride'),\n","    ('husband', 'wife'),\n","    ('grandfather', 'grandmother'),\n","    ('grandson', 'granddaughter'),\n","    ('widower', 'widow'),\n","    ('master', 'mistress'),\n","    ('host', 'hostess'),\n","    ('actor', 'actress'),\n","    ('waiter', 'waitress'),\n","    ('steward', 'stewardess'),\n","    ('nephew', 'niece'),\n","    ('wizard', 'witch'),\n","    ('hero', 'heroine'),\n","    ('bachelor', 'spinster'),\n","    ('lad', 'lass'),\n","    ('monk', 'nun'),\n","    ('policeman', 'policewoman'),\n","    ('fireman', 'firewoman'),\n","    ('salesman', 'saleswoman'),\n","    ('mailman', 'mailwoman'),\n","    ('businessman', 'businesswoman'),\n","    ('chairman', 'chairwoman'),\n","    ('priest', 'priestess'),\n","    ('sir', 'madam'),\n","    ('lord', 'lady'),\n","    ('gentleman', 'lady'),\n","    ('sultan', 'sultana'),\n","    ('bull', 'cow'),\n","    ('ram', 'ewe'),\n","    ('boar', 'sow'),\n","    ('cock', 'hen'),\n","    ('drake', 'duck'),\n","    ('stallion', 'mare'),\n","    ('gander', 'goose'),\n","    ('rooster', 'hen'),\n","    ('lion', 'lioness'),\n","    ('tiger', 'tigress'),\n","    ('leopard', 'leopardess'),\n","    ('goat', 'doe'),\n","    ('buck', 'doe'),\n","    ('billy', 'nanny'),\n","    ('stag', 'hind'),\n","    ('wizard', 'sorceress'),\n","    ('sorcerer', 'witch'),\n","    ('drummer', 'drummeress'),\n","    ('barman', 'barmaid'),\n","    ('farmhand', 'farmgirl'),\n","    ('businessman', 'businesswoman'),\n","    ('gigolo', 'prostitute'),\n","    ('shepherd', 'shepherdess'),\n","    ('governor', 'governess'),\n","    ('prince', 'princess'),\n","    ('waiter', 'waitress'),\n","    ('captain', 'captainess'),\n","    ('lord', 'lady'),\n","    ('male', 'female'),\n","    ('man', 'woman'),\n","    ('boy', 'girl'),\n","    ('gentleman', 'lady'),\n","    ('sir', 'madam'),\n","    ('king', 'queen'),\n","    ('god', 'goddess'),\n","    ('father', 'mother'),\n","    ('son', 'daughter'),\n","    ('brother', 'sister'),\n","    ('uncle', 'aunt')]"]},{"cell_type":"code","execution_count":null,"id":"492774e2","metadata":{"id":"492774e2"},"outputs":[],"source":["# Define ungendered word pairs\n","ungendered_pairs = [\n","    ('child', 'adult'),\n","    ('person', 'individual'),\n","    ('human', 'being'),\n","    ('student', 'learner'),\n","    ('employee', 'worker'),\n","    ('friend', 'companion'),\n","    ('citizen', 'resident'),\n","    ('actor', 'performer'),\n","    ('artist', 'creator'),\n","    ('writer', 'author'),\n","    ('musician', 'performer'),\n","    ('chef', 'cook'),\n","    ('doctor', 'physician'),\n","    ('engineer', 'developer'),\n","    ('scientist', 'researcher'),\n","    ('teacher', 'educator'),\n","    ('lawyer', 'attorney'),\n","    ('manager', 'supervisor'),\n","    ('leader', 'director'),\n","    ('customer', 'client'),\n","    ('patient', 'recipient'),\n","    ('guest', 'visitor'),\n","    ('driver', 'operator'),\n","    ('athlete', 'player'),\n","    ('participant', 'member'),\n","    ('speaker', 'presenter'),\n","    ('listener', 'receiver'),\n","    ('reader', 'consumer'),\n","    ('viewer', 'audience'),\n","    ('buyer', 'shopper'),\n","    ('seller', 'vendor'),\n","    ('traveler', 'explorer'),\n","    ('volunteer', 'helper'),\n","    ('worker', 'laborer'),\n","    ('resident', 'inhabitant'),\n","    ('consumer', 'user'),\n","    ('passenger', 'rider'),\n","    ('patient', 'client'),\n","    ('passerby', 'onlooker'),\n","    ('user', 'customer'),\n","    ('viewer', 'spectator'),\n","    ('student', 'pupil'),\n","    ('manager', 'administrator'),\n","    ('participant', 'attendee'),\n","    ('recipient', 'beneficiary'),\n","    ('reader', 'viewer'),\n","    ('writer', 'editor'),\n","    ('artist', 'performer'),\n","    ('player', 'competitor'),\n","    ('colleague', 'coworker'),\n","    ('neighbor', 'resident'),\n","    ('traveler', 'tourist'),\n","    ('customer', 'consumer'),\n","    ('employee', 'staff'),\n","    ('speaker', 'lecturer'),\n","    ('listener', 'observer'),\n","    ('learner', 'student'),\n","    ('worker', 'employee'),\n","    ('author', 'writer'),\n","    ('creator', 'designer'),\n","    ('chef', 'sous chef'),\n","    ('doctor', 'surgeon'),\n","    ('developer', 'programmer'),\n","    ('researcher', 'scholar'),\n","    ('teacher', 'professor'),\n","    ('director', 'executive'),\n","    ('client', 'customer'),\n","    ('recipient', 'holder'),\n","    ('visitor', 'guest'),\n","    ('operator', 'technician'),\n","    ('player', 'athlete'),\n","    ('member', 'participant'),\n","    ('presenter', 'host'),\n","    ('receiver', 'listener'),\n","    ('consumer', 'shopper'),\n","    ('vendor', 'merchant'),\n","    ('explorer', 'adventurer'),\n","    ('helper', 'assistant')\n","]"]},{"cell_type":"code","execution_count":null,"id":"c0499999","metadata":{"id":"c0499999","outputId":"44ad74da-17a0-48de-d5a5-ab5342186bcd"},"outputs":[{"data":{"text/plain":["78"]},"execution_count":135,"metadata":{},"output_type":"execute_result"}],"source":["len(ungendered_pairs)"]},{"cell_type":"code","execution_count":null,"id":"ec0dfa38","metadata":{"id":"ec0dfa38"},"outputs":[],"source":["def detect_bias(model, gendered_pairs, ungendered_pairs, idx):\n","    # Calculate cosine similarity for gendered word pairs\n","    gendered_similarities = [cosine_similarity([model[word1]], [model[word2]])[0][0] for word1, word2 in gendered_pairs]\n","\n","    # Calculate cosine similarity for non-gendered word pairs\n","    non_gendered_similarities = [cosine_similarity([model[word1]], [model[word2]])[0][0] for word1, word2 in ungendered_pairs]\n","\n","    # Perform t-test to compare the similarity scores\n","    t_statistic, p_value = ttest_ind(gendered_similarities, non_gendered_similarities)\n","\n","    # Compare the p-value to determine significance\n","    if p_value < 0.05:\n","        print(\"Significant difference detected. Gender bias may be present.\")\n","        return idx\n","    else:\n","        print(\"No significant difference detected. Gender bias may not be present.\")\n","        return 0"]},{"cell_type":"code","execution_count":null,"id":"12a2e684","metadata":{"id":"12a2e684","outputId":"d063aac9-10d1-489b-8fa7-c212f5fc9e73"},"outputs":[{"name":"stdout","output_type":"stream","text":["Significant difference detected. Gender bias may be present.\n","Significant difference detected. Gender bias may be present.\n","Significant difference detected. Gender bias may be present.\n","Significant difference detected. Gender bias may be present.\n","Significant difference detected. Gender bias may be present.\n","Significant difference detected. Gender bias may be present.\n","Significant difference detected. Gender bias may be present.\n","Significant difference detected. Gender bias may be present.\n","Significant difference detected. Gender bias may be present.\n"]}],"source":["interval = 8\n","rater2 = []\n","for i in range(int(len(gender_pairs)/interval)):\n","    p1 = gender_pairs[i:i+interval]\n","    p2 = ungendered_pairs[i:i+interval]\n","    rater2.append(detect_bias(model_glove_twitter, p1, p2, i+1))"]},{"cell_type":"code","execution_count":null,"id":"0e2b1547","metadata":{"id":"0e2b1547","outputId":"6c9c9ae2-db4f-4225-feab-6112150d4695"},"outputs":[{"data":{"text/plain":["[1, 2, 3, 4, 5, 6, 7, 8, 9]"]},"execution_count":168,"metadata":{},"output_type":"execute_result"}],"source":["rater2"]},{"cell_type":"code","execution_count":null,"id":"8a95c498","metadata":{"id":"8a95c498"},"outputs":[],"source":["from sklearn.metrics import cohen_kappa_score"]},{"cell_type":"code","execution_count":null,"id":"bb66ef43","metadata":{"id":"bb66ef43"},"outputs":[],"source":["## kappa score ranges from -1 to 1\n","## -1 means no agreement\n","## 1 means complete agreement\n","## 0 means chance agreement\n","kappa = cohen_kappa_score(rater1, rater2)"]},{"cell_type":"code","execution_count":null,"id":"b8b92673","metadata":{"id":"b8b92673","outputId":"7b46abc2-f259-4857-9121-059c3d3514ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Kappa score is 0.7567567567567568\n"]}],"source":["print(f\"Kappa score is {kappa}\")"]},{"cell_type":"markdown","id":"ea2f3a9b","metadata":{"id":"ea2f3a9b"},"source":["#### 3. Semantic textual similarity"]},{"cell_type":"markdown","id":"4ebd0b8e","metadata":{"id":"4ebd0b8e"},"source":["In this section, we use validate gender bias of word2vec using a downstream task: semantic textual similarity task (as in \"Gender Bias in Downstream Task\" in \"2023_NLP2_Bias_Measures_Student_version\" notebook).  "]},{"cell_type":"code","source":["sts_df = pd.read_csv('sts-b.tsv', delimiter='\\t')\n","\n","# Obtains STS-B sentence pairs\n","pairs = []\n","for i in (range(sts_df.shape[0])):\n","    row = dict(sts_df.iloc[i])\n","\n","    pair = {}\n","    pair['sentence1'] = row['pair1-2']\n","    pair['sentence2'] = row['pair1-1']\n","    pair['sentence3'] = row['pair2-1']\n","    pairs.append(pair)"],"metadata":{"id":"n1f1R8M2O8YS","executionInfo":{"status":"ok","timestamp":1685459021109,"user_tz":-120,"elapsed":2026,"user":{"displayName":"junoprent@live.nl","userId":"17636247835891681767"}}},"id":"n1f1R8M2O8YS","execution_count":27,"outputs":[]},{"cell_type":"code","execution_count":28,"id":"7c449c92","metadata":{"id":"7c449c92","executionInfo":{"status":"ok","timestamp":1685459021435,"user_tz":-120,"elapsed":328,"user":{"displayName":"junoprent@live.nl","userId":"17636247835891681767"}}},"outputs":[],"source":["## Create dataset\n","import json\n","\n","data = []\n","\n","# Construct JSON data\n","for pair in pairs:\n","    data.append({\n","        \"sentence1\": pair[\"sentence1\"],\n","        \"sentence2\": pair[\"sentence2\"],\n","        \"sentence3\": pair[\"sentence3\"]\n","    })\n","\n","# Write JSON data to a file\n","with open(\"sentence_pairs.json\", \"w\") as outfile:\n","    json.dump(data, outfile, indent=4)"]},{"cell_type":"code","execution_count":29,"id":"56fd9394","metadata":{"id":"56fd9394","executionInfo":{"status":"ok","timestamp":1685459021436,"user_tz":-120,"elapsed":2,"user":{"displayName":"junoprent@live.nl","userId":"17636247835891681767"}}},"outputs":[],"source":["from gensim import matutils\n","from sklearn.metrics.pairwise import cosine_similarity"]},{"cell_type":"code","execution_count":30,"id":"b52f3427","metadata":{"id":"b52f3427","executionInfo":{"status":"ok","timestamp":1685459021436,"user_tz":-120,"elapsed":2,"user":{"displayName":"junoprent@live.nl","userId":"17636247835891681767"}}},"outputs":[],"source":["## load predefined json file\n","with open(\"sentence_pairs.json\", \"r\") as file:\n","    data = json.load(file)"]},{"cell_type":"code","execution_count":31,"id":"241dc5e1","metadata":{"id":"241dc5e1","executionInfo":{"status":"ok","timestamp":1685459021436,"user_tz":-120,"elapsed":2,"user":{"displayName":"junoprent@live.nl","userId":"17636247835891681767"}}},"outputs":[],"source":["def compute_sim_diff(sent1, sent2, sent3):\n","    # Preprocess sentences\n","    preprocessed_sentence1 = sent1.lower().split()\n","    preprocessed_sentence2 = sent2.lower().split()\n","    preprocessed_sentence3 = sent3.lower().split()\n","\n","    # Compute sentence vectors\n","    sentence_vector1 = np.mean([word2vec_model[word] for word in preprocessed_sentence1 if word in word2vec_model], axis=0)\n","    sentence_vector2 = np.mean([word2vec_model[word] for word in preprocessed_sentence2 if word in word2vec_model], axis=0)\n","    sentence_vector3 = np.mean([word2vec_model[word] for word in preprocessed_sentence3 if word in word2vec_model], axis=0)\n","\n","    # Handle out-of-vocabulary words\n","    sentence_vector1 = np.nan_to_num(sentence_vector1, nan=0.0)\n","    sentence_vector2 = np.nan_to_num(sentence_vector2, nan=0.0)\n","    sentence_vector3 = np.nan_to_num(sentence_vector3, nan=0.0)\n","    \n","    # Calculate cosine similarity\n","    similarity_score1 = cosine_similarity([sentence_vector1], [sentence_vector2])[0][0]\n","    similarity_score2 = cosine_similarity([sentence_vector1], [sentence_vector3])[0][0]\n","    \n","#     print(\"Similarity score:\", similarity_score1-similarity_score2)\n","    return similarity_score1-similarity_score2\n"]},{"cell_type":"code","execution_count":32,"id":"c2461df2","metadata":{"id":"c2461df2","outputId":"07a554e7-9adc-4fbd-eb12-9102552b8b79","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685459037025,"user_tz":-120,"elapsed":15591,"user":{"displayName":"junoprent@live.nl","userId":"17636247835891681767"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["69.9% of all samples demonstrate male orientation\n"]}],"source":["diff = [compute_sim_diff(sample['sentence1'], sample['sentence2'], sample['sentence3']) for sample in data] \n","ratio = len([i for i in diff if i>0])*100/len(diff)\n","print(f\"{ratio:.1f}% of all samples demonstrate male orientation\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}